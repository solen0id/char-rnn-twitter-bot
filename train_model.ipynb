{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation, Embedding, TimeDistributed\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, TensorBoard\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "ALLOWED_CHARS = [' ', '!', '?', ',', '.', '\\'', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "def load_text(fpath):\n",
    "    path = Path(fpath)\n",
    "    text = path.read_text().lower()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    return  \"\".join([char for char in text if char in ALLOWED_CHARS])\n",
    "\n",
    "print(\"loading text...\")\n",
    "text = load_text(\"input.txt\")\n",
    "text = clean_text(text)\n",
    "\n",
    "print(\"indexing characters...\")\n",
    "char_to_idx = {char:idx for idx, char in enumerate(sorted(set(text)))}   # string to set splits string into chars\n",
    "idx_to_char = {idx:char for char, idx in char_to_idx.items()}\n",
    "joblib.dump(char_to_idx, \"models/char_to_idx_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### build model ###\n",
    "print(\"building model...\")\n",
    "\n",
    "SEQ_LEN = 100\n",
    "STEP = 5\n",
    "LAYER_COUNT = 1\n",
    "HIDDEN_LAYER_SIZE = 128\n",
    "DROPOUT = 0.2\n",
    "VOCAB_SIZE = len(char_to_idx)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(VOCAB_SIZE, HIDDEN_LAYER_SIZE, input_shape=(SEQ_LEN,)))\n",
    "\n",
    "for i in range(LAYER_COUNT):\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            HIDDEN_LAYER_SIZE,\n",
    "            return_sequences=True,\n",
    "            stateful=False,\n",
    "        )\n",
    "    )\n",
    "    model.add(Dropout(DROPOUT))\n",
    "\n",
    "model.add(TimeDistributed(Dense(VOCAB_SIZE)))\n",
    "model.add(Activation(\"softmax\"))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\")\n",
    "\n",
    "### define model callbacks ###\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "\n",
    "    def sample(preds, temperature=1.0):\n",
    "        # helper function to sample an index from a probability array\n",
    "        preds = np.asarray(preds).astype('float64')\n",
    "        preds = np.log(preds) / temperature\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / np.sum(exp_preds)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        return np.argmax(probas)\n",
    "\n",
    "\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "    start_index = random.randint(0, len(text) - SEQ_LEN - 1)\n",
    "    for diversity in [0.2, 0.5, 1.0, 1.2]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        sequence = np.asarray([char_to_idx[c] for c in text[start_index: start_index + SEQ_LEN]])\n",
    "        seed = ''.join([idx_to_char[idx] for idx in sequence])\n",
    "\n",
    "        print('----- Generating with seed:\\n\"{}\"\\n'.format(seed))\n",
    "        print(seed, end='')\n",
    "\n",
    "        for i in range(200):\n",
    "            preds = model.predict(sequence.reshape(-1,SEQ_LEN), verbose=0)\n",
    "            next_char_index = sample(preds[0][-1], diversity)\n",
    "            next_char = idx_to_char[next_char_index]\n",
    "\n",
    "            sequence = np.append(sequence[1:], next_char_index)\n",
    "            print(next_char, end='')\n",
    "\n",
    "        print('\\n\\n')\n",
    "\n",
    "tensorboard_callback = TensorBoard(log_dir='./logs')\n",
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "checkpoint_callback = ModelCheckpoint('./models/char_model_demo', monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks=[tensorboard_callback, print_callback, checkpoint_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences_w_targets(text, seq_len, step):\n",
    "    n_samples = int((len(text) - seq_len) / step) + 1\n",
    "    X, y = np.zeros((n_samples, seq_len)), np.zeros((n_samples, seq_len, VOCAB_SIZE))\n",
    "\n",
    "    for i in range(0, len(text) - seq_len, step):\n",
    "        append_idx = int(i/step)\n",
    "        char_seq = text[i:i+seq_len]\n",
    "        next_char = text[i+seq_len]\n",
    "        char_seq_shifted = char_seq[1:] + next_char\n",
    "        char_seq_as_one_hot = [to_categorical(idx, num_classes=VOCAB_SIZE) for idx\n",
    "                               in [char_to_idx[c] for c in char_seq_shifted]]\n",
    "        \n",
    "        X[append_idx] = np.asarray([char_to_idx[c] for c in char_seq])\n",
    "        y[append_idx] = np.asarray(char_seq_as_one_hot)\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### generate trainingdata ###\n",
    "print(\"generating char sequences for training...\")\n",
    "\n",
    "X, y = make_sequences_w_targets(text[:10000], SEQ_LEN, STEP)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"beginning training...\")\n",
    "model.fit(X, y, batch_size=64, epochs=10, callbacks=callbacks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
